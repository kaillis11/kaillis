#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ì‡¼í•‘ ìŠ¤í…”ìŠ¤ í¬ë¡¤ëŸ¬ v2.0
ìµœì‹  ìš°íšŒ ê¸°ë²•ê³¼ ë” ì •êµí•œ ë¸Œë¼ìš°ì € ì‹œë®¬ë ˆì´ì…˜
"""

import requests
import time
import random
import json
from bs4 import BeautifulSoup
from urllib.parse import quote, urlencode
import re

class StealthNaverCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.setup_stealth_session()
        
    def setup_stealth_session(self):
        """ìµœì‹  ìŠ¤í…”ìŠ¤ í—¤ë” ì„¤ì •"""
        # ìµœì‹  Chrome í—¤ë”
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br, zstd',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'sec-ch-ua': '"Chromium";v="122", "Not(A:Brand";v="24", "Google Chrome";v="122"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'Cache-Control': 'max-age=0',
            'DNT': '1'
        }
        self.session.headers.update(headers)
        
    def human_delay(self, min_delay=1, max_delay=3):
        """ì¸ê°„ì ì¸ ë”œë ˆì´ íŒ¨í„´"""
        delay = random.uniform(min_delay, max_delay)
        time.sleep(delay)
        
    def get_naver_cookies(self):
        """ë„¤ì´ë²„ ë©”ì¸ì—ì„œ ì¿ í‚¤ íšë“"""
        try:
            print("ğŸª ë„¤ì´ë²„ ë©”ì¸í˜ì´ì§€ ì ‘ì†í•˜ì—¬ ì¿ í‚¤ íšë“...")
            response = self.session.get('https://www.naver.com', timeout=10)
            if response.status_code == 200:
                print("âœ… ì¿ í‚¤ íšë“ ì„±ê³µ!")
                return True
            else:
                print(f"âŒ ì¿ í‚¤ íšë“ ì‹¤íŒ¨: {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ ì¿ í‚¤ íšë“ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
            
    def alternative_search_url(self, query):
        """ëŒ€ì•ˆì ì¸ ê²€ìƒ‰ URL ìƒì„±"""
        # ëª¨ë°”ì¼ í˜ì´ì§€ ìš°ì„  ì‹œë„
        encoded_query = quote(query)
        
        urls = [
            f"https://msearch.shopping.naver.com/search/all?query={encoded_query}",
            f"https://search.shopping.naver.com/search/all?query={encoded_query}&sort=REVIEW",
            f"https://search.shopping.naver.com/search/all?query={encoded_query}&sort=POPULAR",
            f"https://search.shopping.naver.com/api/search/all?query={encoded_query}"
        ]
        
        return urls
        
    def try_multiple_approaches(self, query, limit=10):
        """ì—¬ëŸ¬ ì ‘ê·¼ ë°©ë²• ì‹œë„"""
        print(f"ğŸ¯ '{query}' ê²€ìƒ‰ì„ ìœ„í•œ ë‹¤ì¤‘ ì ‘ê·¼ ì‹œë„...")
        
        # 1. ë¨¼ì € ì¿ í‚¤ íšë“
        if not self.get_naver_cookies():
            print("âš ï¸ ì¿ í‚¤ ì—†ì´ ì§„í–‰...")
            
        self.human_delay(2, 4)
        
        # 2. ì—¬ëŸ¬ URL ì‹œë„
        urls = self.alternative_search_url(query)
        
        for i, url in enumerate(urls, 1):
            print(f"\nğŸ”„ ì ‘ê·¼ ë°©ë²• {i}/{len(urls)} ì‹œë„...")
            print(f"ğŸ“ URL: {url}")
            
            try:
                # Referer ì„¤ì • (ìì—°ìŠ¤ëŸ¬ìš´ ì ‘ê·¼ì²˜ëŸ¼)
                if i > 1:
                    self.session.headers['Referer'] = 'https://www.naver.com'
                
                response = self.session.get(url, timeout=15)
                print(f"ğŸ“Š ìƒíƒœ ì½”ë“œ: {response.status_code}")
                print(f"ğŸ“ ì‘ë‹µ í¬ê¸°: {len(response.content)} bytes")
                
                if response.status_code == 200:
                    # HTML íŒŒì‹± ì‹œë„
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # ìƒí’ˆ ìš”ì†Œ ì°¾ê¸° (ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„)
                    selectors = [
                        'div[data-testid="basicList_item_list"] > div',
                        '.product_item',
                        '.item_area',
                        '.goods_area',
                        '[class*="item"]',
                        'a[data-i]'
                    ]
                    
                    products_found = False
                    for selector in selectors:
                        items = soup.select(selector)
                        if items and len(items) > 0:
                            print(f"âœ… ìƒí’ˆ ë°œê²¬! ì…€ë ‰í„°: {selector}, ê°œìˆ˜: {len(items)}")
                            products_found = True
                            break
                    
                    if products_found:
                        return self.extract_products(soup, limit)
                    else:
                        print("ğŸ” ìƒí’ˆ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        # í˜ì´ì§€ ë‚´ìš© ì¼ë¶€ ì¶œë ¥
                        if len(response.text) > 100:
                            print(f"ğŸ“„ í˜ì´ì§€ ë‚´ìš© ìƒ˜í”Œ: {response.text[:200]}...")
                        
                elif response.status_code == 418:
                    print("ğŸ¤– 418 ì—ëŸ¬: ë´‡ìœ¼ë¡œ ì¸ì‹ë¨")
                else:
                    print(f"âŒ HTTP ì—ëŸ¬: {response.status_code}")
                    
            except Exception as e:
                print(f"âŒ ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}")
                
            # ë‹¤ìŒ ì‹œë„ ì „ ë”œë ˆì´
            if i < len(urls):
                self.human_delay(3, 6)
                
        print("\nâŒ ëª¨ë“  ì ‘ê·¼ ë°©ë²• ì‹¤íŒ¨")
        return []
        
    def extract_products(self, soup, limit):
        """ìƒí’ˆ ì •ë³´ ì¶”ì¶œ"""
        products = []
        
        # ë‹¤ì–‘í•œ ìƒí’ˆ ì…€ë ‰í„° ì‹œë„
        selectors = [
            'div[data-testid="basicList_item_list"] > div',
            '.product_item',
            '.item_area',
            '.goods_area'
        ]
        
        items = []
        for selector in selectors:
            items = soup.select(selector)
            if items:
                break
                
        if not items:
            print("âŒ ìƒí’ˆ í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return products
            
        print(f"ğŸ” {len(items)}ê°œ ìƒí’ˆ í•­ëª© ë°œê²¬")
        
        for i, item in enumerate(items[:limit]):
            try:
                # ìƒí’ˆëª… ì¶”ì¶œ
                title_selectors = [
                    '.product_title',
                    '.item_title',
                    'a[data-i]',
                    'h3',
                    'h4',
                    '.goods_name'
                ]
                
                title = None
                for sel in title_selectors:
                    title_elem = item.select_one(sel)
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        break
                        
                # ê°€ê²© ì¶”ì¶œ
                price_selectors = [
                    '.price_num',
                    '.item_price',
                    '.goods_price',
                    '[class*="price"]'
                ]
                
                price = None
                for sel in price_selectors:
                    price_elem = item.select_one(sel)
                    if price_elem:
                        price_text = price_elem.get_text(strip=True)
                        # ìˆ«ìë§Œ ì¶”ì¶œ
                        price_numbers = re.findall(r'[\d,]+', price_text)
                        if price_numbers:
                            price = price_numbers[0].replace(',', '')
                            break
                
                if title:
                    product = {
                        'rank': i + 1,
                        'title': title,
                        'price': price or 'ê°€ê²© ì •ë³´ ì—†ìŒ'
                    }
                    products.append(product)
                    print(f"  {i+1}. {title} - {product['price']}")
                    
            except Exception as e:
                print(f"âš ï¸ ìƒí’ˆ {i+1} íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue
                
        return products

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='ë„¤ì´ë²„ì‡¼í•‘ ìŠ¤í…”ìŠ¤ í¬ë¡¤ëŸ¬')
    parser.add_argument('query', help='ê²€ìƒ‰í•  ìƒí’ˆ')
    parser.add_argument('--limit', type=int, default=10, help='ìµœëŒ€ ìƒí’ˆ ìˆ˜')
    
    args = parser.parse_args()
    
    crawler = StealthNaverCrawler()
    products = crawler.try_multiple_approaches(args.query, args.limit)
    
    if products:
        print(f"\nğŸ‰ ì´ {len(products)}ê°œ ìƒí’ˆ í¬ë¡¤ë§ ì„±ê³µ!")
        for product in products:
            print(f"{product['rank']}. {product['title']} - {product['price']}")
    else:
        print("\nğŸ˜ í¬ë¡¤ë§ëœ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()