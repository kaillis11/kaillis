#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ì‡¼í•‘ Cloudflare ìš°íšŒ í¬ë¡¤ëŸ¬
ì•„í…Œë‚˜ì˜ cloudscraper ë°©ë²•ë¡ ì„ ë„¤ì´ë²„ì— ì ìš©
"""

import cloudscraper
from bs4 import BeautifulSoup
import json
import time
import random
from urllib.parse import quote
import re

class CloudflareNaverCrawler:
    def __init__(self):
        """Cloudflare ìš°íšŒ ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”"""
        print("ğŸ”¥ Cloudflare ìš°íšŒ ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”...")
        self.scraper = cloudscraper.create_scraper()
        
        # ì¶”ê°€ í—¤ë” ì„¤ì •
        self.scraper.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
    def human_delay(self, min_delay=1, max_delay=3):
        """ì¸ê°„ì ì¸ ë”œë ˆì´"""
        delay = random.uniform(min_delay, max_delay)
        time.sleep(delay)
        
    def get_naver_cookies(self):
        """ë„¤ì´ë²„ ë©”ì¸ì—ì„œ ì¿ í‚¤ íšë“"""
        try:
            print("ğŸª ë„¤ì´ë²„ ë©”ì¸í˜ì´ì§€ ì ‘ì†í•˜ì—¬ ì¿ í‚¤ íšë“...")
            response = self.scraper.get('https://www.naver.com', timeout=10)
            if response.status_code == 200:
                print("âœ… ì¿ í‚¤ íšë“ ì„±ê³µ!")
                return True
            else:
                print(f"âŒ ì¿ í‚¤ íšë“ ì‹¤íŒ¨: {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ ì¿ í‚¤ íšë“ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
            
    def search_naver_shopping(self, query, limit=10):
        """ë„¤ì´ë²„ì‡¼í•‘ ê²€ìƒ‰ (Cloudflare ìš°íšŒ)"""
        print(f"ğŸ›ï¸ ë„¤ì´ë²„ì‡¼í•‘ '{query}' ê²€ìƒ‰ ì‹œì‘...")
        
        # 1. ë¨¼ì € ì¿ í‚¤ íšë“
        self.get_naver_cookies()
        self.human_delay(2, 4)
        
        # 2. ê²€ìƒ‰ URL ìƒì„±
        encoded_query = quote(query)
        urls = [
            f"https://search.shopping.naver.com/search/all?query={encoded_query}",
            f"https://search.shopping.naver.com/search/all?query={encoded_query}&sort=POPULAR",
            f"https://search.shopping.naver.com/search/all?query={encoded_query}&sort=REVIEW"
        ]
        
        for i, url in enumerate(urls, 1):
            print(f"\nğŸ”„ ì‹œë„ {i}/{len(urls)}: {url}")
            
            try:
                # Cloudflare ìš°íšŒ ìš”ì²­
                response = self.scraper.get(url, timeout=15)
                
                print(f"ğŸ“Š ìƒíƒœ ì½”ë“œ: {response.status_code}")
                print(f"ğŸ“ ì‘ë‹µ í¬ê¸°: {len(response.content)} bytes")
                
                if response.status_code == 200:
                    print("âœ… ì ‘ì† ì„±ê³µ! HTML íŒŒì‹± ì‹œì‘...")
                    
                    # HTML íŒŒì‹±
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # í˜ì´ì§€ ë‚´ìš© ìƒ˜í”Œ ì¶œë ¥
                    page_text = soup.get_text()[:500]
                    print(f"ğŸ“„ í˜ì´ì§€ ë‚´ìš© ìƒ˜í”Œ:\n{page_text}...")
                    
                    # ìƒí’ˆ ìš”ì†Œ ì°¾ê¸°
                    products = self.extract_products(soup, limit)
                    
                    if products:
                        return products
                    else:
                        print("ğŸ” ìƒí’ˆ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ, ë‹¤ìŒ URL ì‹œë„...")
                        
                elif response.status_code == 418:
                    print("ğŸ¤– 418 ì—ëŸ¬: ì—¬ì „íˆ ë´‡ìœ¼ë¡œ ì¸ì‹ë¨")
                else:
                    print(f"âŒ HTTP ì—ëŸ¬: {response.status_code}")
                    
            except Exception as e:
                print(f"âŒ ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}")
                
            # ë‹¤ìŒ ì‹œë„ ì „ ë”œë ˆì´
            if i < len(urls):
                self.human_delay(3, 6)
                
        print("\nâŒ ëª¨ë“  ì‹œë„ ì‹¤íŒ¨")
        return []
        
    def extract_products(self, soup, limit):
        """ìƒí’ˆ ì •ë³´ ì¶”ì¶œ"""
        products = []
        
        # ë‹¤ì–‘í•œ ìƒí’ˆ ì…€ë ‰í„° ì‹œë„
        selectors = [
            'div[data-testid="basicList_item_list"] > div',
            '.product_item',
            '.item_area',
            '.goods_area',
            '[class*="item"]',
            'a[data-i]',
            '.product_link',
            '.basicList_item',
            '.item'
        ]
        
        items = []
        used_selector = None
        
        for selector in selectors:
            items = soup.select(selector)
            if items and len(items) > 2:  # ìµœì†Œ 3ê°œ ì´ìƒ ìˆì–´ì•¼ ìœ íš¨
                used_selector = selector
                break
                
        if not items:
            print("âŒ ìƒí’ˆ í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return products
            
        print(f"ğŸ” '{used_selector}'ë¡œ {len(items)}ê°œ í•­ëª© ë°œê²¬")
        
        for i, item in enumerate(items[:limit]):
            try:
                # ìƒí’ˆëª… ì¶”ì¶œ (ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„)
                title_selectors = [
                    '.product_title',
                    '.item_title', 
                    '.goods_name',
                    'a[data-i]',
                    'h3',
                    'h4',
                    '[class*="title"]',
                    '[class*="name"]'
                ]
                
                title = None
                for sel in title_selectors:
                    title_elem = item.select_one(sel)
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        if title and len(title) > 2:  # ìœ íš¨í•œ ì œëª©ì¸ì§€ í™•ì¸
                            break
                            
                # ê°€ê²© ì¶”ì¶œ
                price_selectors = [
                    '.price_num',
                    '.item_price',
                    '.goods_price',
                    '[class*="price"]',
                    '.num'
                ]
                
                price = None
                for sel in price_selectors:
                    price_elem = item.select_one(sel)
                    if price_elem:
                        price_text = price_elem.get_text(strip=True)
                        # ìˆ«ìë§Œ ì¶”ì¶œ
                        price_numbers = re.findall(r'[\d,]+', price_text)
                        if price_numbers:
                            price = price_numbers[0].replace(',', '')
                            break
                
                if title:
                    product = {
                        'rank': i + 1,
                        'title': title,
                        'price': price or 'ê°€ê²© ì •ë³´ ì—†ìŒ'
                    }
                    products.append(product)
                    print(f"  {i+1}. {title} - {product['price']}")
                    
            except Exception as e:
                print(f"âš ï¸ ìƒí’ˆ {i+1} íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue
                
        return products

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='ë„¤ì´ë²„ì‡¼í•‘ Cloudflare ìš°íšŒ í¬ë¡¤ëŸ¬')
    parser.add_argument('query', help='ê²€ìƒ‰í•  ìƒí’ˆ')
    parser.add_argument('--limit', type=int, default=10, help='ìµœëŒ€ ìƒí’ˆ ìˆ˜')
    
    args = parser.parse_args()
    
    crawler = CloudflareNaverCrawler()
    products = crawler.search_naver_shopping(args.query, args.limit)
    
    if products:
        print(f"\nğŸ‰ ì´ {len(products)}ê°œ ìƒí’ˆ í¬ë¡¤ë§ ì„±ê³µ!")
        print("\nğŸ“‹ ì•„ì´ìŠ¤í¬ë¦¼ ìˆœìœ„:")
        for product in products:
            print(f"{product['rank']}. {product['title']} - {product['price']}")
    else:
        print("\nğŸ˜ í¬ë¡¤ë§ëœ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()