#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
11ë²ˆê°€ HTML êµ¬ì¡° ë¶„ì„ ë„êµ¬
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import quote

def analyze_11st_structure():
    """11ë²ˆê°€ í˜ì´ì§€ êµ¬ì¡° ë¶„ì„"""
    
    # ì„¸ì…˜ ì„¤ì •
    session = requests.Session()
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    session.headers.update(headers)
    
    # 11ë²ˆê°€ ê²€ìƒ‰
    query = "ì•„ì´ìŠ¤í¬ë¦¼"
    encoded_query = quote(query)
    search_url = f"https://search.11st.co.kr/Search.tmall?kwd={encoded_query}"
    
    print(f"ğŸ” 11ë²ˆê°€ êµ¬ì¡° ë¶„ì„ ì‹œì‘...")
    print(f"ğŸ“ URL: {search_url}")
    
    try:
        response = session.get(search_url, timeout=15)
        print(f"ğŸ“Š ìƒíƒœ ì½”ë“œ: {response.status_code}")
        print(f"ğŸ“ ì‘ë‹µ í¬ê¸°: {len(response.content)} bytes")
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # HTML êµ¬ì¡° ìƒ˜í”Œ ì €ì¥
            with open('11st_sample.html', 'w', encoding='utf-8') as f:
                f.write(soup.prettify())
            print("ğŸ“„ HTML ìƒ˜í”Œ ì €ì¥: 11st_sample.html")
            
            # í´ë˜ìŠ¤ ì´ë¦„ë“¤ ì°¾ê¸°
            print("\nğŸ” ë°œê²¬ëœ í´ë˜ìŠ¤ë“¤:")
            all_classes = set()
            for element in soup.find_all(True):
                if element.get('class'):
                    for cls in element.get('class'):
                        all_classes.add(cls)
            
            # ìƒí’ˆ ê´€ë ¨ í´ë˜ìŠ¤ í•„í„°ë§
            product_classes = [cls for cls in all_classes if any(keyword in cls.lower() for keyword in 
                             ['product', 'item', 'prd', 'goods', 'list', 'search', 'result'])]
            
            print("ğŸ“¦ ìƒí’ˆ ê´€ë ¨ í´ë˜ìŠ¤ë“¤:")
            for cls in sorted(product_classes)[:20]:  # ìƒìœ„ 20ê°œë§Œ
                print(f"  .{cls}")
            
            # div íƒœê·¸ë“¤ì˜ í´ë˜ìŠ¤ ë¶„ì„
            print("\nğŸ—ï¸ div íƒœê·¸ í´ë˜ìŠ¤ ë¶„ì„:")
            div_classes = set()
            for div in soup.find_all('div'):
                if div.get('class'):
                    for cls in div.get('class'):
                        if any(keyword in cls.lower() for keyword in ['product', 'item', 'prd', 'goods']):
                            div_classes.add(cls)
            
            for cls in sorted(div_classes)[:15]:
                print(f"  div.{cls}")
            
            # ì‹¤ì œ ìƒí’ˆ ê²€ìƒ‰ (í…ìŠ¤íŠ¸ ê¸°ë°˜)
            print("\nğŸ¦ ì•„ì´ìŠ¤í¬ë¦¼ ê´€ë ¨ í…ìŠ¤íŠ¸ ì°¾ê¸°:")
            page_text = soup.get_text()
            icecream_keywords = ['ë©”ë¡œë‚˜', 'í•˜ê²ë‹¤ì¦ˆ', 'ë¶•ì–´ì‹¸ë§Œì½”', 'ìŠˆí¼ì½˜', 'ë¼ì§€ë°”', 'ì•„ì´ìŠ¤í¬ë¦¼']
            
            found_keywords = []
            for keyword in icecream_keywords:
                if keyword in page_text:
                    found_keywords.append(keyword)
            
            if found_keywords:
                print(f"âœ… ë°œê²¬ëœ í‚¤ì›Œë“œ: {', '.join(found_keywords)}")
                print("ğŸ’¡ ì‹¤ì œ ìƒí’ˆ ë°ì´í„°ê°€ í˜ì´ì§€ì— ìˆìŒ!")
            else:
                print("âŒ ì•„ì´ìŠ¤í¬ë¦¼ ê´€ë ¨ í‚¤ì›Œë“œ ì—†ìŒ")
                print("ğŸ¤” ë™ì  ë¡œë”©ì´ê±°ë‚˜ ë‹¤ë¥¸ êµ¬ì¡°ì¼ ìˆ˜ ìˆìŒ")
            
            # í˜ì´ì§€ ì†ŒìŠ¤ ì¼ë¶€ ì¶œë ¥
            print(f"\nğŸ“„ í˜ì´ì§€ ë‚´ìš© ìƒ˜í”Œ (ì²« 1000ì):")
            print(page_text[:1000])
            
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    analyze_11st_structure()