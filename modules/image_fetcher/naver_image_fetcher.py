#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ì‡¼í•‘ API ê¸°ë°˜ ì œí’ˆ ì´ë¯¸ì§€ ìˆ˜ì§‘ê¸° v1.0
ì¿ íŒ¡ ë­í‚¹ ë°ì´í„°ì™€ ì—°ë™í•˜ì—¬ ì œí’ˆ ì´ë¯¸ì§€ë¥¼ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜¤ëŠ” ì‹œìŠ¤í…œ
"""

import time
import random
import json
import os
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import requests
from urllib.parse import urlencode, quote
from typing import List, Dict, Optional

class NaverImageFetcher:
    def __init__(self, headless=True):
        """ë„¤ì´ë²„ ì‡¼í•‘ ì´ë¯¸ì§€ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”"""
        self.setup_driver(headless)
        self.base_url = "https://search.shopping.naver.com/search?"
        
    def setup_driver(self, headless=True):
        """Selenium ë“œë¼ì´ë²„ ì„¤ì •"""
        chrome_options = Options()
        
        # WSL í™˜ê²½ ìµœì í™” ì„¤ì •
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-web-security')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # User-Agent ì„¤ì •
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        chrome_options.add_argument(f'--user-agent={random.choice(user_agents)}')
        
        # Chromium ë°”ì´ë„ˆë¦¬ ì„¤ì • (ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„)
        possible_binaries = [
            '/usr/bin/chromium-browser',
            '/usr/bin/chromium',
            '/usr/bin/google-chrome',
            '/usr/bin/google-chrome-stable'
        ]
        
        chrome_binary = None
        for binary in possible_binaries:
            if os.path.exists(binary):
                chrome_binary = binary
                break
        
        if chrome_binary:
            chrome_options.binary_location = chrome_binary
            print(f"ğŸ”§ Chrome ë°”ì´ë„ˆë¦¬ ì°¾ìŒ: {chrome_binary}")
        else:
            print("âš ï¸ Chrome ë°”ì´ë„ˆë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ ì‚¬ìš©")
            
        self.driver = webdriver.Chrome(options=chrome_options)
        
        # ì°¨ë‹¨ ìš°íšŒ
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
    def search_product_image(self, product_name: str, max_attempts: int = 3) -> Optional[str]:
        """ì œí’ˆëª…ìœ¼ë¡œ ë„¤ì´ë²„ ì‡¼í•‘ì—ì„œ ì´ë¯¸ì§€ ê²€ìƒ‰"""
        print(f"ğŸ” ì´ë¯¸ì§€ ê²€ìƒ‰: {product_name}")
        
        # ê²€ìƒ‰ì–´ ì •ì œ (ë¸Œëœë“œëª…ê³¼ ì£¼ìš” í‚¤ì›Œë“œë§Œ ì¶”ì¶œ)
        clean_query = self._clean_product_name(product_name)
        print(f"   ì •ì œëœ ê²€ìƒ‰ì–´: {clean_query}")
        
        for attempt in range(max_attempts):
            try:
                # ë„¤ì´ë²„ ì‡¼í•‘ ê²€ìƒ‰ URL ìƒì„±
                params = {
                    'query': clean_query,
                    'cat_id': '',
                    'frm': 'NVSCTAB'
                }
                search_url = self.base_url + urlencode(params, quote_via=quote)
                
                print(f"   ì‹œë„ {attempt + 1}: {search_url}")
                self.driver.get(search_url)
                
                # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                time.sleep(random.uniform(2, 4))
                
                # ì²« ë²ˆì§¸ ìƒí’ˆ ì´ë¯¸ì§€ ì°¾ê¸°
                wait = WebDriverWait(self.driver, 10)
                
                # ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ ëŒ€ê¸°
                try:
                    products_container = wait.until(
                        EC.presence_of_element_located((By.CLASS_NAME, "basicList_list_basis__uNBZx"))
                    )
                except:
                    print(f"   âŒ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ë¡œë”© ì‹¤íŒ¨")
                    continue
                
                # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                
                # ì²« ë²ˆì§¸ ìƒí’ˆ ì´ë¯¸ì§€ ì¶”ì¶œ
                image_url = self._extract_first_image(soup)
                
                if image_url:
                    print(f"   âœ… ì´ë¯¸ì§€ ë°œê²¬: {image_url[:50]}...")
                    return image_url
                else:
                    print(f"   âš ï¸ ì´ë¯¸ì§€ ì—†ìŒ - ì¬ì‹œë„")
                    time.sleep(random.uniform(1, 2))
                    
            except Exception as e:
                print(f"   âŒ ê²€ìƒ‰ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
                time.sleep(random.uniform(2, 3))
                
        print(f"   ğŸ˜ {max_attempts}ë²ˆ ì‹œë„ í›„ ì´ë¯¸ì§€ ì°¾ê¸° ì‹¤íŒ¨")
        return None
    
    def _clean_product_name(self, product_name: str) -> str:
        """ì œí’ˆëª… ì •ì œ - ë¸Œëœë“œëª…ê³¼ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ"""
        # ì£¼ìš” ë¸Œëœë“œëª…ê³¼ ì œí’ˆ í‚¤ì›Œë“œ ì¶”ì¶œ
        import re
        
        # ë¸Œëœë“œëª… íŒ¨í„´
        brand_patterns = [
            r'(ë§ˆì¼“ì˜¤|Market O)',
            r'(ë°°ìŠ¤í‚¨ë¼ë¹ˆìŠ¤|Baskin Robbins)',
            r'(ë‰´íŠ¸ë¦¬ì˜¤ì½”|Nutrioko)',
            r'(ì¿ ìº£|KUCAT)',
            r'(ë„ë‹´|Neoldam)',
            r'(ì ¤ë¦¬ì ¤ë¦¬|Jelly Jelly)', 
            r'(ë‹¤ë„¤ì‹œíƒ€|Daneshita)',
            r'(ë§¤ì¼ìœ ì—…|Maeil)',
            r'(ë˜í‚¨|Dunkin)',
            r'(í—ˆì‰¬|Hershey)'
        ]
        
        # í•µì‹¬ ì œí’ˆ í‚¤ì›Œë“œ
        product_keywords = [
            'ë¸Œë¼ìš°ë‹ˆ', 'ë§ì°¨', 'íŒŒì´', 'ëª¨ì°Œ', 'ì›¨ì´í¼', 'ì°¹ìŒ€ë–¡', 'ì¹´ìŠ¤í…Œë¼', 
            'ì¿ í‚¤', 'ì´ˆì½œë¦¿', 'ë””ì €íŠ¸', 'ì•„ì´ìŠ¤í¬ë¦¼', 'ì¼€ì´í¬', 'ë„ë„›'
        ]
        
        found_brand = ""
        found_keywords = []
        
        # ë¸Œëœë“œëª… ì°¾ê¸°
        for pattern in brand_patterns:
            match = re.search(pattern, product_name, re.IGNORECASE)
            if match:
                found_brand = match.group(1)
                break
        
        # í•µì‹¬ í‚¤ì›Œë“œ ì°¾ê¸°
        for keyword in product_keywords:
            if keyword in product_name:
                found_keywords.append(keyword)
        
        # ì •ì œëœ ê²€ìƒ‰ì–´ ì¡°í•©
        if found_brand and found_keywords:
            clean_query = f"{found_brand} {' '.join(found_keywords[:2])}"
        elif found_brand:
            clean_query = found_brand
        elif found_keywords:
            clean_query = ' '.join(found_keywords[:2])
        else:
            # ì²« ë²ˆì§¸ ë‹¨ì–´ë§Œ ì‚¬ìš©
            clean_query = product_name.split()[0] if product_name.split() else product_name
            
        return clean_query
    
    def _extract_first_image(self, soup: BeautifulSoup) -> Optional[str]:
        """ì²« ë²ˆì§¸ ìƒí’ˆ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        try:
            # ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ì„ íƒì ì‹œë„
            image_selectors = [
                'img.image_item__1T4eB',  # ê¸°ë³¸ ìƒí’ˆ ì´ë¯¸ì§€
                'img.product_link__TrAac img',  # ë§í¬ ë‚´ë¶€ ì´ë¯¸ì§€
                '.basicList_item__30_LI img',  # ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ì´ë¯¸ì§€
                '.product_item__MDtDF img',  # ì œí’ˆ ì•„ì´í…œ ì´ë¯¸ì§€
                'img[alt*="ìƒí’ˆ"]',  # alt í…ìŠ¤íŠ¸ì— "ìƒí’ˆ" í¬í•¨
                'img[src*="shopping"]'  # srcì— "shopping" í¬í•¨
            ]
            
            for selector in image_selectors:
                images = soup.select(selector)
                if images:
                    for img in images:
                        src = img.get('src') or img.get('data-src')
                        if src and self._is_valid_image_url(src):
                            return src
                            
        except Exception as e:
            print(f"   âŒ ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            
        return None
    
    def _is_valid_image_url(self, url: str) -> bool:
        """ìœ íš¨í•œ ì´ë¯¸ì§€ URLì¸ì§€ í™•ì¸"""
        if not url:
            return False
            
        # ê¸°ë³¸ ì¡°ê±´ë“¤
        valid_conditions = [
            url.startswith('http'),  # HTTP/HTTPSë¡œ ì‹œì‘
            any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']),  # ì´ë¯¸ì§€ í™•ì¥ì
            'shop' in url.lower(),  # ì‡¼í•‘ ê´€ë ¨ URL
            len(url) > 20  # ì¶©ë¶„í•œ ê¸¸ì´
        ]
        
        # ì œì™¸í•  URL íŒ¨í„´
        exclude_patterns = [
            'logo', 'banner', 'icon', 'button', 'arrow', 'star'
        ]
        
        url_lower = url.lower()
        if any(pattern in url_lower for pattern in exclude_patterns):
            return False
            
        return all(valid_conditions)
    
    def process_ranking_data(self, json_file_path: str) -> Dict:
        """ì¿ íŒ¡ ë­í‚¹ JSON íŒŒì¼ì„ ì½ì–´ì„œ ì´ë¯¸ì§€ ì¶”ê°€"""
        print(f"ğŸ“Š ë­í‚¹ ë°ì´í„° ì²˜ë¦¬: {json_file_path}")
        
        # JSON íŒŒì¼ ì½ê¸°
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except FileNotFoundError:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_file_path}")
            return {}
        except json.JSONDecodeError:
            print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {json_file_path}")
            return {}
        
        ranking = data.get('ranking', [])
        print(f"ğŸ“‹ ì´ {len(ranking)}ê°œ ì œí’ˆ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì‹œì‘...")
        
        # ê° ì œí’ˆë³„ ì´ë¯¸ì§€ ìˆ˜ì§‘
        success_count = 0
        for i, product in enumerate(ranking):
            print(f"\n{i+1}/{len(ranking)} ì²˜ë¦¬ ì¤‘...")
            
            product_name = product.get('name', '')
            if not product_name:
                print("   âš ï¸ ì œí’ˆëª…ì´ ì—†ì–´ ìŠ¤í‚µ")
                continue
            
            # ì´ë¯¸ì§€ ê²€ìƒ‰
            image_url = self.search_product_image(product_name)
            
            if image_url:
                product['image_url'] = image_url
                product['image_status'] = 'found'
                success_count += 1
            else:
                product['image_url'] = None
                product['image_status'] = 'not_found'
            
            # ì´ë¯¸ì§€ ì²˜ë¦¬ ì •ë³´ ì¶”ê°€
            product['image_processed_at'] = datetime.now().isoformat()
            
            # ë”œë ˆì´ (ì°¨ë‹¨ ë°©ì§€)
            if i < len(ranking) - 1:  # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´
                delay = random.uniform(3, 6)
                print(f"   ğŸ’¤ {delay:.1f}ì´ˆ ëŒ€ê¸°...")
                time.sleep(delay)
        
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        data['meta']['image_processing'] = {
            'processed_at': datetime.now().isoformat(),
            'total_products': len(ranking),
            'images_found': success_count,
            'success_rate': f"{(success_count/len(ranking)*100):.1f}%" if ranking else "0%"
        }
        
        print(f"\nğŸ¯ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì™„ë£Œ!")
        print(f"   ğŸ“Š ì„±ê³µë¥ : {success_count}/{len(ranking)} ({(success_count/len(ranking)*100):.1f}%)")
        
        return data
    
    def save_enhanced_data(self, data: Dict, output_suffix: str = "_with_images") -> str:
        """ì´ë¯¸ì§€ê°€ ì¶”ê°€ëœ ë°ì´í„° ì €ì¥"""
        if not data:
            print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return ""
        
        # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        category = data.get('meta', {}).get('category', 'products')
        filename = f"enhanced_{category}_ranking{output_suffix}_{timestamp}.json"
        
        # ë°ì´í„° ì €ì¥
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ í–¥ìƒëœ ë°ì´í„° ì €ì¥: {filename}")
            return filename
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def close(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if hasattr(self, 'driver') and self.driver:
            self.driver.quit()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ–¼ï¸ ë„¤ì´ë²„ ì‡¼í•‘ ì´ë¯¸ì§€ ìˆ˜ì§‘ê¸° v1.0 ì‹œì‘!")
    
    # ê¸°ì¡´ ì¿ íŒ¡ ë­í‚¹ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    json_file = "/mnt/d/ai/project_hub/active_projects/WhatToEat/modules/data_parser/coupang_dessert_ranking_20250709_122527.json"
    
    if not os.path.exists(json_file):
        print(f"âŒ ë­í‚¹ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {json_file}")
        return
    
    # ì´ë¯¸ì§€ ìˆ˜ì§‘ê¸° ìƒì„±
    fetcher = NaverImageFetcher(headless=True)
    
    try:
        # ë­í‚¹ ë°ì´í„°ì— ì´ë¯¸ì§€ ì¶”ê°€
        enhanced_data = fetcher.process_ranking_data(json_file)
        
        if enhanced_data:
            # í–¥ìƒëœ ë°ì´í„° ì €ì¥
            output_file = fetcher.save_enhanced_data(enhanced_data)
            
            if output_file:
                print(f"\nğŸ‰ ì‘ì—… ì™„ë£Œ!")
                print(f"   ğŸ“ ê²°ê³¼ íŒŒì¼: {output_file}")
                print(f"   ğŸ”— ë‹¤ìŒ ë‹¨ê³„: WhatToEat ë£°ë ›ì— ì´ë¯¸ì§€ ì—°ë™")
        
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
    
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        fetcher.close()
        print("ğŸ”š ì´ë¯¸ì§€ ìˆ˜ì§‘ê¸° ì¢…ë£Œ")

if __name__ == "__main__":
    main()