#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ì‡¼í•‘ ë””ì €íŠ¸ í¬ë¡¤ë§ ì‹œìŠ¤í…œ (ê³ ê¸‰ ìš°íšŒ ë²„ì „)
requests + ê³ ê¸‰ í—¤ë” ì„¤ì •ìœ¼ë¡œ ë´‡ ì°¨ë‹¨ ìš°íšŒ
"""

import requests
import time
import random
import pandas as pd
from bs4 import BeautifulSoup
import json

class AdvancedNaverCrawler:
    def __init__(self):
        """ê³ ê¸‰ ìš°íšŒ ê¸°ëŠ¥ì„ ê°€ì§„ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.session = requests.Session()
        self.setup_session()
        self.products = []
        
    def setup_session(self):
        """ì„¸ì…˜ ì„¤ì • - ì‹¤ì œ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ë„ë¡"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'DNT': '1',
            'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"'
        }
        self.session.headers.update(headers)
        
        # ì¿ í‚¤ ì„¤ì • (ë„¤ì´ë²„ ì ‘ê·¼ ê¸°ë¡ì´ ìˆëŠ” ê²ƒì²˜ëŸ¼)
        self.session.cookies.update({
            'NNB': 'RANDOM_VALUE',
            'nx_ssl': 'Y'
        })
        
    def get_naver_homepage_first(self):
        """ë¨¼ì € ë„¤ì´ë²„ í™ˆí˜ì´ì§€ì— ì ‘ì†í•´ì„œ ì¿ í‚¤ë¥¼ ì–»ìŒ"""
        try:
            print("ğŸ  ë„¤ì´ë²„ í™ˆí˜ì´ì§€ ì ‘ì† ì¤‘...")
            response = self.session.get('https://www.naver.com', timeout=10)
            if response.status_code == 200:
                print("âœ… ë„¤ì´ë²„ í™ˆí˜ì´ì§€ ì ‘ì† ì„±ê³µ!")
                time.sleep(random.uniform(1, 3))
                return True
            else:
                print(f"âŒ ë„¤ì´ë²„ í™ˆí˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ ë„¤ì´ë²„ í™ˆí˜ì´ì§€ ì ‘ì† ì¤‘ ì˜¤ë¥˜: {e}")
            return False
            
    def crawl_dessert_products(self, max_products=20):
        """ë„¤ì´ë²„ì‡¼í•‘ ë””ì €íŠ¸ ì œí’ˆ í¬ë¡¤ë§"""
        # 1ë‹¨ê³„: ë„¤ì´ë²„ í™ˆí˜ì´ì§€ ë¨¼ì € ì ‘ì†
        if not self.get_naver_homepage_first():
            print("âŒ ì‚¬ì „ ì ‘ì† ì‹¤íŒ¨")
            return []
        
        # 2ë‹¨ê³„: ì‡¼í•‘ ë©”ì¸ í˜ì´ì§€ ì ‘ì†
        shopping_main = "https://shopping.naver.com"
        try:
            print("ğŸ›ï¸ ë„¤ì´ë²„ì‡¼í•‘ ë©”ì¸ ì ‘ì† ì¤‘...")
            response = self.session.get(shopping_main, timeout=10)
            if response.status_code == 200:
                print("âœ… ë„¤ì´ë²„ì‡¼í•‘ ë©”ì¸ ì ‘ì† ì„±ê³µ!")
                time.sleep(random.uniform(2, 4))
            else:
                print(f"âš ï¸ ë„¤ì´ë²„ì‡¼í•‘ ë©”ì¸ ì ‘ì† ì‹¤íŒ¨: {response.status_code}")
        except Exception as e:
            print(f"âš ï¸ ë„¤ì´ë²„ì‡¼í•‘ ë©”ì¸ ì ‘ì† ì¤‘ ì˜¤ë¥˜: {e}")
        
        # 3ë‹¨ê³„: ê²€ìƒ‰ í˜ì´ì§€ ì ‘ì†
        search_url = "https://search.shopping.naver.com/search/all"
        params = {
            'query': 'ë””ì €íŠ¸',
            'cat_id': '',
            'frm': 'NVSHATC'
        }
        
        try:
            print(f"ğŸ” ë””ì €íŠ¸ ê²€ìƒ‰ ì¤‘...")
            
            # Referer í—¤ë” ì¶”ê°€
            self.session.headers.update({
                'Referer': 'https://shopping.naver.com/'
            })
            
            response = self.session.get(search_url, params=params, timeout=15)
            
            print(f"ğŸ“Š ì‘ë‹µ ìƒíƒœ: {response.status_code}")
            print(f"ğŸ“ ì‘ë‹µ í¬ê¸°: {len(response.text)} bytes")
            
            if response.status_code == 200:
                print("âœ… ê²€ìƒ‰ í˜ì´ì§€ ë¡œë“œ ì„±ê³µ!")
                
                # ì‘ë‹µ ë‚´ìš© ì¼ë¶€ í™•ì¸
                if len(response.text) > 1000:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    products = self.extract_products_advanced(soup, max_products)
                    return products
                else:
                    print("âŒ ì‘ë‹µ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ì°¨ë‹¨ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤)")
                    return []
            else:
                print(f"âŒ ê²€ìƒ‰ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {response.status_code}")
                if response.status_code == 418:
                    print("ğŸ¤– ë´‡ìœ¼ë¡œ ì¸ì‹ë˜ì–´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
                elif response.status_code == 403:
                    print("ğŸš« ì ‘ê·¼ì´ ê¸ˆì§€ë˜ì—ˆìŠµë‹ˆë‹¤")
                return []
                
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
    
    def extract_products_advanced(self, soup, max_products):
        """ê³ ê¸‰ ì œí’ˆ ì •ë³´ ì¶”ì¶œ"""
        products = []
        
        # í˜ì´ì§€ êµ¬ì¡° ë¶„ì„
        print("ğŸ” í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ ì¤‘...")
        
        # ë‹¤ì–‘í•œ ì„ íƒì íŒ¨í„´ ì‹œë„
        selectors_to_try = [
            # ìµœì‹  ë„¤ì´ë²„ì‡¼í•‘ êµ¬ì¡°
            'div[data-testid="basicList"] > div',
            '.basicList_item__0T9JD',
            '.basicList_item__2XT81', 
            '.product_item__1XD8w',
            '.adProduct_item__1zC9h',
            # ì¼ë°˜ì ì¸ ìƒí’ˆ ì»¨í…Œì´ë„ˆ
            '.item',
            '[class*="item"]',
            '[class*="product"]',
            # ë§í¬ ê¸°ë°˜
            'a[href*="/shopping/"]',
            'a[href*="nvMid"]'
        ]
        
        product_elements = []
        used_selector = None
        
        for selector in selectors_to_try:
            elements = soup.select(selector)
            if elements and len(elements) >= 3:  # ìµœì†Œ 3ê°œ ì´ìƒ ì°¾ì•„ì•¼ ìœ ì˜ë¯¸
                product_elements = elements
                used_selector = selector
                print(f"âœ… ì œí’ˆ ìš”ì†Œ ë°œê²¬: {selector} ({len(elements)}ê°œ)")
                break
        
        if not product_elements:
            print("âŒ ì œí’ˆ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            # ë””ë²„ê¹…ì„ ìœ„í•œ HTML êµ¬ì¡° ì¶œë ¥
            print("\nğŸ“‹ HTML êµ¬ì¡° ìƒ˜í”Œ:")
            sample_divs = soup.find_all('div', limit=10)
            for div in sample_divs:
                if div.get('class'):
                    print(f"  div.{'.'.join(div['class'])}")
            return []
        
        print(f"ğŸ¯ ì„ íƒëœ ì„ íƒì: {used_selector}")
        
        # ì œí’ˆ ì •ë³´ ì¶”ì¶œ
        for idx, element in enumerate(product_elements[:max_products]):
            try:
                product = self.extract_single_product(element, idx + 1)
                if product and product.get('name') and product['name'] != f"ì œí’ˆ {idx + 1}":
                    products.append(product)
                    print(f"âœ… {len(products)}. {product['name'][:40]}...")
                
                # ì²˜ë¦¬ ê°„ ë”œë ˆì´
                time.sleep(random.uniform(0.1, 0.3))
                
            except Exception as e:
                print(f"âŒ ì œí’ˆ {idx+1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        return products
    
    def extract_single_product(self, element, rank):
        """ê°œë³„ ì œí’ˆ ì •ë³´ ì¶”ì¶œ"""
        product = {'rank': rank}
        
        # ì œí’ˆëª… ì¶”ì¶œ (ë‹¤ì–‘í•œ íŒ¨í„´)
        name_patterns = [
            '.product_title',
            '.productTitle',
            '.title',
            'h3', 'h4', 'h5',
            '[class*="title"]',
            '[class*="name"]',
            'a[title]'  # title ì†ì„±ì—ì„œ ì œí’ˆëª… ì¶”ì¶œ
        ]
        
        name = None
        for pattern in name_patterns:
            elem = element.select_one(pattern)
            if elem:
                name = elem.get_text(strip=True)
                if not name and elem.get('title'):  # title ì†ì„± ì²´í¬
                    name = elem['title'].strip()
                if name and len(name) > 2:  # ìœ ì˜ë¯¸í•œ ê¸¸ì´
                    break
        
        product['name'] = name if name else f"ì œí’ˆ {rank}"
        
        # ê°€ê²© ì¶”ì¶œ
        price_patterns = [
            '.price_num',
            '.price',
            '.cost',
            '[class*="price"]',
            '[class*="cost"]',
            '.num'
        ]
        
        price = None
        for pattern in price_patterns:
            elem = element.select_one(pattern)
            if elem:
                price_text = elem.get_text(strip=True)
                # ìˆ«ìê°€ í¬í•¨ëœ ê²½ìš°ë§Œ
                if any(char.isdigit() for char in price_text):
                    price = price_text
                    break
        
        product['price'] = price if price else "ê°€ê²© ì •ë³´ ì—†ìŒ"
        
        # ë¦¬ë·° ì •ë³´ ì¶”ì¶œ
        review_patterns = [
            '.review',
            '.count',
            '[class*="review"]',
            '[class*="count"]',
            '.etc'
        ]
        
        review = None
        for pattern in review_patterns:
            elem = element.select_one(pattern)
            if elem:
                review_text = elem.get_text(strip=True)
                if review_text and ('ë¦¬ë·°' in review_text or any(char.isdigit() for char in review_text)):
                    review = review_text
                    break
        
        product['reviews'] = review if review else "ë¦¬ë·° ì •ë³´ ì—†ìŒ"
        
        # ë§í¬ ì¶”ì¶œ
        link_elem = element.find('a', href=True)
        if link_elem:
            href = link_elem['href']
            if href.startswith('//'):
                href = 'https:' + href
            elif href.startswith('/'):
                href = 'https://search.shopping.naver.com' + href
            product['link'] = href
        else:
            product['link'] = "ë§í¬ ì—†ìŒ"
        
        return product
    
    def save_to_csv(self, products, filename="naver_dessert_advanced.csv"):
        """ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥"""
        if not products:
            print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        df = pd.DataFrame(products)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filename} ({len(products)}ê°œ ì œí’ˆ)")
        
        # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        print("\nğŸ“‹ í¬ë¡¤ë§ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
        for i, product in enumerate(products[:3]):
            print(f"{i+1}. ğŸ“¦ {product['name'][:50]}")
            print(f"   ğŸ’° {product['price']}")
            print(f"   â­ {product['reviews']}")
            print(f"   ğŸ”— {product['link'][:80]}...")
            print()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë„¤ì´ë²„ì‡¼í•‘ ë””ì €íŠ¸ í¬ë¡¤ë§ ì‹œì‘! (ê³ ê¸‰ ìš°íšŒ ë²„ì „)")
    
    crawler = AdvancedNaverCrawler()
    products = crawler.crawl_dessert_products(max_products=10)  # ë¨¼ì € 10ê°œë¡œ í…ŒìŠ¤íŠ¸
    
    if products:
        crawler.save_to_csv(products)
        print(f"ğŸ‰ ì„±ê³µ! {len(products)}ê°œ ì œí’ˆ í¬ë¡¤ë§ ì™„ë£Œ!")
    else:
        print("âŒ í¬ë¡¤ë§ëœ ì œí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë„¤ì´ë²„ì‡¼í•‘ì˜ ë´‡ ì°¨ë‹¨ì´ ê°•í™”ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    print("ğŸ”š í¬ë¡¤ë§ ì™„ë£Œ!")

if __name__ == "__main__":
    main()